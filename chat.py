import streamlit as st
from anthropic import Anthropic

max_input_token=40000

client = Anthropic(api_key=st.secrets['ANTHROPIC_API_KEY'])

def claude_stream_generator(response_stream):
    """Claude APIì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ í…ìŠ¤íŠ¸ ì œë„ˆë ˆì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    for chunk in response_stream:
        if hasattr(chunk, 'type'):
            # content_block_delta ì´ë²¤íŠ¸ ì²˜ë¦¬
            if chunk.type == 'content_block_delta' and hasattr(chunk, 'delta') and hasattr(chunk.delta, 'text'):
                yield chunk.delta.text
            # content_block_start ì´ë²¤íŠ¸ ì²˜ë¦¬
            elif chunk.type == 'content_block_start' and hasattr(chunk, 'content_block') and hasattr(chunk.content_block, 'text'):
                yield chunk.content_block.text

def get_preview_with_claude(messages):
    user_messages = [m['content'] for m in messages if m.get('role') == 'user']
    message_in_string = "\n".join(f"- {msg}" for msg in user_messages[:5]) 

    prompt = f"""ë‹¤ìŒ ëŒ€í™”ì˜ ì œëª©ì„ í•œê¸€ 10ì ì´ë‚´ ë˜ëŠ” ì˜ì–´ 20ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì œëª©ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. 
               {message_in_string}
              ì œëª©:"""
    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=64,
        temperature=0.2,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.content[0].text.strip().split('\n')[0]

#ì…ë ¥ í† í° ì¹´ìš´íŒ…
def count_token(model, system, messages):
    response = client.messages.count_tokens(
        model=model,
        system=system,
        messages=messages,
    )
    return int(dict(response)['input_tokens'])


def truncate_messages(messages, system_prompt, max_tokens=max_input_token):
    """í† í° ì‚¬ìš©ëŸ‰ ì¶”ì‚°ì„ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ ëŒ€í™” ê¸¸ì´ ì œí•œ"""
    if len(messages) == 0:
        return messages

    # í˜„ì¬ ì „ì²´ í† í° ìˆ˜ ê³„ì‚°
    current_tokens = count_token("claude-sonnet-4-20250514", system_prompt, messages)

    # í† í° ìˆ˜ê°€ ì œí•œ ì´í•˜ë©´ ì „ì²´ ë°˜í™˜
    if current_tokens <= max_tokens:
        return messages, current_tokens

    # í† í° ìˆ˜ê°€ ì´ˆê³¼í•˜ë©´ ë¹„ë¡€ì ìœ¼ë¡œ ëŒ€í™” ìˆ˜ ì¤„ì´ê¸°
    total_conversations = len(messages) // 2  # user+assistant ìŒì˜ ê°œìˆ˜
    if total_conversations == 0:
        return messages, current_tokens

    # ìœ ì§€í•  ëŒ€í™” ìˆ˜ ê³„ì‚° (ìµœì†Œ 1ê°œëŠ” ë³´ì¥)
    keep_conversations = max(1, int(total_conversations * (max_tokens / current_tokens)))

    # ìµœê·¼ Nê°œ ëŒ€í™”ë§Œ ìœ ì§€ (user+assistant ìŒ ë‹¨ìœ„)
    keep_messages_count = keep_conversations * 2
    truncated_messages = messages[-keep_messages_count:]
    return truncated_messages, int(current_tokens * (max_tokens / current_tokens))
        
def generate_claude_response(model, temperature, system_prompt):
    # ë©”ì‹œì§€ ê¸°ë¡ ì¤€ë¹„
    messages = [
        {"role": m["role"], "content": m["content"]}
        for m in st.session_state.messages
    ]
    truncated_messages, num_input_tokens = truncate_messages(messages, system_prompt, max_tokens=max_input_token)
    st.session_state.num_input_tokens = num_input_tokens
    
    # ì‘ë‹µ ê´€ë ¨ ë³€ìˆ˜ë“¤ì„ ë¯¸ë¦¬ ì´ˆê¸°í™”
    full_response = ""
    response_placeholder = None
    
    try:
        # API í˜¸ì¶œ
        with st.spinner("Claudeê°€ ì‘ë‹µ ì¤‘..."):
            # ìƒˆë¡œìš´ chat_message ì»¨í…Œì´ë„ˆ ìƒì„±
            with st.chat_message("assistant"):
                # ì´ˆê¸° í…ìŠ¤íŠ¸ë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •
                response_placeholder = st.empty()
                response_placeholder.markdown("")
                
                response = client.messages.create(
                    model=model,
                    messages=truncated_messages,
                    temperature=temperature,
                    max_tokens=64000,
                    system=system_prompt,
                    stream=True
                )
                
                # ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° (ë” ì•ˆì „í•œ ì²˜ë¦¬)
                try:
                    for text in claude_stream_generator(response):
                        full_response += text
                        # ì‘ë‹µ ì—…ë°ì´íŠ¸ (ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€)
                        try:
                            response_placeholder.markdown(full_response)
                        except Exception as display_error:
                            # í™”ë©´ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                            print(f"Display update failed: {display_error}")
                            continue
                            
                except Exception as stream_error:
                    print(f"Streaming error: {stream_error}")
                    # ìŠ¤íŠ¸ë¦¬ë°ì´ ì¤‘ë‹¨ë˜ì–´ë„ ì§€ê¸ˆê¹Œì§€ ë°›ì€ ì‘ë‹µì€ ì €ì¥
                    if full_response.strip():
                        st.warning("ì‘ë‹µ ì¤‘ ì—°ê²°ì´ ëŠì–´ì¡Œì§€ë§Œ, ë¶€ë¶„ ì‘ë‹µì„ ì €ì¥í•©ë‹ˆë‹¤.")
                    else:
                        raise stream_error  # ì•„ë¬´ê²ƒë„ ë°›ì§€ ëª»í–ˆìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ
                
                # ìµœì¢… ì‘ë‹µ í‘œì‹œ
                if response_placeholder and full_response:
                    response_placeholder.markdown(full_response)
            
            # ë©”ì‹œì§€ ê¸°ë¡ì— ì¶”ê°€ (ì‘ë‹µì´ ìˆì„ ë•Œë§Œ)
            if full_response.strip():
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                print(f"Response saved to history: {len(full_response)} characters")
            else:
                st.error("ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                
        # ì‘ë‹µ ìƒì„± ì™„ë£Œ
        st.session_state.generating_response = False
        
    except Exception as e:
        # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ë¶€ë¶„ ì‘ë‹µì´ ìˆìœ¼ë©´ ì €ì¥
        if full_response.strip():
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            st.warning(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì§€ë§Œ ë¶€ë¶„ ì‘ë‹µì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {str(e)}")
        else:
            # ì‘ë‹µì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ì˜¤ë¥˜ ì²˜ë¦¬
            if 'overloaded_error' in str(e):
                st.error("ì´ëŸ°, Anthropic ì„œë²„ê°€ ì£½ì–´ìˆë„¤ìš”ğŸ˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•´ ì£¼ì„¸ìš”")
            else:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        
        st.session_state.generating_response = False
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸
        print(f"Exception in generate_claude_response: {str(e)}")
        print(f"Full response at exception: '{full_response}'")        
